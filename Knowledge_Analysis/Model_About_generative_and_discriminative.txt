总览：
	决策函数Y=f(X)						AND			条件概率分布P(Y|X)  【在X条件下Y的概率】
	生成方法(generative approach)		AND			判别方法(discriminative approach)
	生成模型(generative model)			AND			判别模型(discriminative model)

【知识点】条件概率：是指事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为：P(A|B)，读作“在B条件下A的概率
【知识点】联合概率：是指两个事件同时发生的概率，P(A,B),AB同时发生的概率

一、决策函数和条件概率分布
决策函数：
	决策函数Y=f(X)：你输入一个X，它就输出一个Y，这个Y与一个阈值比较，根据比较结果判定X属于哪个类别。
		例如两类（w1和w2）分类问题，如果Y大于阈值，X就属于类w1，如果小于阈值就属于类w2。这样就得到了该X对应的类别了

条件概率分布函数：		
	你输入一个X，它通过比较它属于所有类的概率，然后输出概率最大的那个作为该X对应的类别。
		例如：如果P(w1|X)大于P(w2|X)，那么我们就认为X是属于w1类的。
		在X的条件下，w1的概率大于w2的概率，认为，X属于w1类

二、生成方法和判别方法
生成方法：
	由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)。基本
		思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类，就像上面说的那样。
		
	解析：（即，先求出P(X)的概率，然后求P(X,Y)的概率，然后计算出在P(X)的条件下，P(X,Y)的概率，就是P(Y|X)了）
		注意了哦，这里是先求出P(X,Y)才得到P(Y|X)的，然后这个过程还得先求出P(X)。P(X)就是你的训练数据的概率分布。哎，刚才说了，
			需要你的数据样本非常多的时候，你得到的P(X)才能很好的描述你数据真正的分布。例如你投硬币，你试了100次，得到正面的次
			数和你的试验次数的比可能是3/10，然后你直觉告诉你，可能不对，然后你再试了500次，哎，这次正面的次数和你的试验次数的
			比可能就变成4/10，这时候你半信半疑，不相信上帝还有一个手，所以你再试200000次，这时候正面的次数和你的试验次数的比（
			就可以当成是正面的概率了）就变成5/10了。这时候，你就觉得很靠谱了，觉得自己就是那个上帝了。呵呵，真啰嗦，还差点离题了

判别方法：（即，直接得到了P(Y|X),不用去研究怎么来的，可以直接拿来用，训练后直接用于预测了）
	由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。基本思想是有限样本条件下建立判别函数，
		不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括k近邻，感知机，决策树，支持向量机等

三、优缺点
生成方法的特点：上面说到，生成方法学习联合概率密度分布P(X,Y)，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似
	度。但它不关心到底划分各类的那个分类边界在哪。生成方法可以还原出联合概率分布P(Y|X)，而判别方法不能。生成方法的学习收敛速度更快，
	即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用

判别方法的特点：判别方法直接学习的是决策函数Y=f(X)或者条件概率分布P(Y|X)。不能反映训练数据本身的特性。但它寻找不同类别之间的最优分
	类面，反映的是异类数据之间的差异。直接面对预测，往往学习的准确率更高。由于直接学习P(Y|X)或P(X)，可以对数据进行各种程度上的抽象、
	定义特征并使用特征，因此可以简化学习问题

由生成模型可以得到判别模型，但由判别模型得不到生成模型。